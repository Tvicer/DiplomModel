# -*- coding: utf-8 -*-
"""Копия блокнота "Diplom.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10OSW7j-PewBeP7bmzYRz68qEOqtnqm-2

# Загрузка и импортирование необходимых библиотек

[0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0
 1 0 0 1 1 1 1 0 1 1 1 0 0]


0            0
1            0
2            0
3            1
4            0
5            1
6            0
7            0
8            0
9            0
10           1
11           0
12           1
13           0
14           1
15           0
16           1
17           1
18           0
19           0
20           0
21           1
22           1
23           0
24           0
25           1
26           1
27           1
28           0
29           0
30           0
31           0
32           1
33           0
34           1
35           0
36           1
37           1
38           1
39           0
40           0
41           0
42           1
43           1
44           0
45           0
46           0
47           0
48           1
49           1

 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0]
"""

from __future__ import annotations

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import re

import seaborn as sns
import nltk
import requests

from string import punctuation
from collections import Counter, namedtuple
from itertools import product
from typing import Dict, List
from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering
from sklearn import preprocessing

from bs4 import BeautifulSoup

from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, \
                    NewsMorphTagger, NewsSyntaxParser
from razdel import sentenize, tokenize
from nltk.corpus import stopwords

from natasha import (
    Segmenter, MorphVocab,
    NewsEmbedding, NewsMorphTagger, NewsSyntaxParser,
    Doc
)
import natasha
import ipymarkup
from typing import Any, List, Dict, Tuple, Optional, Set
import os
import json
import re

from itertools import combinations

from enum import Enum

nltk.download("stopwords")

"""# Предобработка данных"""

path_data_speech = 'dataset_speech.xlsx'
df = pd.read_excel(path_data_speech, sheet_name=3, skiprows=[1], usecols=[5,6,7])
df.head()

"""Переименуем названия столбцов, удалим пустые строки"""

df.columns = ["tell", "tell_time", "answer"]
df.drop(index=df[pd.isnull(df["tell"])].index, axis=1, inplace=True)
df[["tell", "tell_time", "answer"]] = df[["tell", "tell_time", "answer"]].astype(str)
df['tell_time'] = df['tell_time'].map(lambda s: re.sub('\D', '', s))
df

df.info()

"""## Функции для обработки текста"""

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)


def tokenize_wrapper(in_str: str):
  """
  Токенизация текста с помощью Наташи
  """
  return [item.text for item in tokenize(in_str) if item.text not in punctuation]


def sentenize_wrapper(in_str: str):
  """
  Разделения текста на предложения с помощью Наташи
  """
  return [sent.text for sent in sentenize(in_str)]


def get_lemms(in_str: str):
  """
  Разделяем полученный текст на токены и получаем лемму для каждого
  """
  curr_doc = Doc(in_str)
  curr_doc.segment(segmenter)
  curr_doc.tag_morph(morph_tagger)
  for token in curr_doc.tokens:
    token.lemmatize(morph_vocab)
  return [token.lemma for token in curr_doc.tokens if token.lemma not in punctuation]


def get_pos(in_str: str):
  """
  Получим части речи для каждого токена в предложении
  """
  curr_doc = Doc(in_str)
  curr_doc.segment(segmenter)
  curr_doc.tag_morph(morph_tagger)
  return [(token.text, token.pos) for token in curr_doc.tokens]

"""# Графики

Вспомогательные функции
"""

def get_statisctics(l, text=None):
  text = '' if text is None else text.strip()+' '

  print(f"MAX {text}- {np.max(l)}")
  print(f"MIN {text}- {np.min(l)}")
  print(f"MEAN {text}- {np.mean(l)}")
  print(f"MEDIAN {text}- {np.median(l)}")

"""Количество слов в текстах"""

count_lemmas = list(map(lambda x: len(tokenize_wrapper(x)), df.tell.values))

sns.boxplot(x=count_lemmas).set_title('Количество слов в текстах')

plt.figure(figsize=(10,5))
sns.violinplot(x=count_lemmas).set_title('Количество слов в текстах')

get_statisctics(count_lemmas, text='токенов в рассказе')

"""Количество предложений в текстах"""

count_sentences = list(map(lambda x: len(sentenize_wrapper(x)), df.tell.values))

plt.figure(figsize=(10,5))
sns.boxplot(x=count_sentences).set_title('Количество предложений в текстах')

get_statisctics(count_sentences, text='токенов в рассказе')

"""Посмотрим на выбросы"""

df.tell.values[np.array(list(map(lambda x: len(sentenize_wrapper(x)), df.tell.values))) == 10]

"""# Предобработка текста"""

class SentencesPreprocessor():
  """
      Standart sentence normalizer
  """

  def __init__(self, stopword_lang="russian", min_token_size=3):
    self._russian_stopwords = stopwords.words('russian')

    self._min_token_size = min_token_size
    self._inverse_word = ['не', 'нет', 'нету', 'ни']
    self._increase_word = ['очень']

    # self._morph_vocab = MorphVocab()
    # self._morph_analyzer = pymorphy2.MorphAnalyzer()


  def normalize_sentence(self, sentence):
    """
    Standart sentense normalizer

    :param sentence: input sentence
    :return: normalized sentence
    """
    curr_doc = Doc(sentence)
    curr_doc.segment(segmenter)
    curr_doc.tag_morph(morph_tagger)
    for token in curr_doc.tokens:
      token.lemmatize(morph_vocab)
    return [token.lemma.replace('ё', 'е') for token in curr_doc.tokens if token.lemma not in punctuation \
              and token.lemma.strip().isalpha() and len(token.lemma) >= self._min_token_size]

  def normalize_iter(self, iter):
    l = []
    for word in iter:
      prep = self.normalize_sentence(word)
      if prep:
        l.append(prep[0])
    return l

  def transform(self, df: pd.DataFrame, col_sentence='tell'):
    """
    Using Natasha make preprocessing operation for your dataframe of sentences

    :param df:
    :param col_sentence:
    :return:
    """
    df[col_sentence] = df[col_sentence].apply(self.normalize_sentence_via_mystem)
    return df

preprocess = SentencesPreprocessor().normalize_sentence

"""Уникальные слова"""

def get_word_uniq(text, prep=True):
  """
  На вход получаем текст, а возвращаем словарь с подсчетом уникальных токенов
  """
  return Counter(preprocess(text) if prep else tokenize_wrapper(text))

def get_word_uniq_lst(lst_texts, prep=True):
  """
  На вход получаем список текстов, а возвращаем словарь с подсчетом уникальных токенов
  """
  all_word_lst = []
  preprocessor = SentencesPreprocessor()
  for text in lst_texts:
    all_word_lst.extend(preprocess(text) if prep else tokenize_wrapper(text))
  return Counter(all_word_lst)

counter = get_word_uniq_lst(df.tell.values)
counter.most_common(20)

count_uniq_lemmas = list(map(lambda x: len(get_word_uniq(x)), df.tell.values))

sns.boxplot(x=count_uniq_lemmas).set_title('Количество уникальных слов в текстах')

counter = get_word_uniq_lst(df.tell.values, prep=False)
counter.most_common(20)

"""# Параметры оценки

1. Смысловая полнота
2. Смысловая адекватность А и Б
3. Программирование
4. Грамматическое оформление
5. Количество параграмматизмов
6. Длина рассказа (количество слов и предложений)
7. Количество сложных предложений
8. Средняя длина фразы
9. Правильные ответы на вопросы
10. Лексическое оформление
11. Разнообразие словаря (TTR)
12. Прономинализация
13. Количество атрибутов (прилагательных и наречий)
14. Неузуальная инверсия
15. Составные глагольные сказуемые
16. Незаполненные валентности
17. Страдательный залог (+возвратные глаголы)
18. Логические коннекторы
19. Критерий «Ситуативность»
20. Критерий «Причинность»
21. Соблюдение нарративной структуры «Цель – попытка – результат»
22. Тип нарратива: полный / сокращенный / искаженный

# Синонимы 1 (так себе)

https://habr.com/ru/post/661629/
Использует вики
"""

# !pip install sparqlwrapper

import requests

# session = requests.Session()
# URL = 'https://www.wikidata.org/w/api.php'

# def wbgetentities(name):
#     res = session.post(URL, data={
#         'action': 'wbsearchentities',
#         'search': name,
#         'language':'ru',
#         'format': 'json',
#     })
#     try:
#         res_json = res.json()['search'][0]['id']
#     except:
#         res_json = None
#     return res_json

# Q_id = wbgetentities('собака')

# from SPARQLWrapper import SPARQLWrapper, JSON

# sparql = SPARQLWrapper(
#     'https://query.wikidata.org/sparql'
# )
# sparql.setReturnFormat(JSON)

# def create_query(first_id):
#     q = ('''
#     PREFIX entity: <http://www.wikidata.org/entity/>
#     PREFIX wdt: <http://www.wikidata.org/prop/direct/>
#     SELECT ?syno
#     WHERE {
#       ?O ?P ?id .
#       OPTIONAL{?id skos:altLabel ?syno
#           filter (lang(?syno) = 'ru')}
#       VALUES ?id {entity:'''+ first_id +'''}
#       SERVICE wikibase:label {bd:serviceParam wikibase:language "ru" .}}''')
#     return q

# query = create_query(Q_id)

# sparql.setQuery(query)

# try:
#     ret = sparql.queryAndConvert()

#     for r in ret["results"]["bindings"]:
#         print(r)
# except Exception as e:
#     print(e)

"""# Синонимы 2

ru_wordnet
"""


from wiki_ru_wordnet import WikiWordnet
from typing import Set

class Synonymizer():
  def __init__(self):
    self.wikiwordnet = WikiWordnet()

  def get_synset_word(self, synset):
    return [w.lemma() for w in synset.get_words()]

  def get_synset(self, word):
    synsets = self.wikiwordnet.get_synsets(word)
    return synsets[0] if synsets else None

  def get_hyponyms(self, synset1):
    if synset1 is None:
      return []

    hyponyms = []
    for hyponym in self.wikiwordnet.get_hyponyms(synset1):
      for w in hyponym.get_words():
        hyponyms.append(self.get_synset(w.lemma()))

    return hyponyms

  def get_hypernyms(self, synset1):
    if synset1 is None:
      return []

    hypernyms = []
    for hypernym in self.wikiwordnet.get_hypernyms(synset1):
      for w in hypernym.get_words():
        hypernyms.append(self.get_synset(w.lemma()))

    return hypernyms

  def get_synonyms(self, word) -> Set:
    words = SentencesPreprocessor().normalize_sentence(word)
    if len(words) == 0:
      return set()

    word = words[0]
    synsets = set()
    synset = self.get_synset(word)
    if synset is not None:
      synsets.add(synset)

    for hyponym1 in self.get_hyponyms(self.get_synset(word)):
      synsets.add(hyponym1)
      for hyponym2 in self.get_hyponyms(hyponym1):
        synsets.add(hyponym2)
        for hyponym3 in self.get_hyponyms(hyponym2):
          synsets.add(hyponym3)

    for hypernym1 in self.get_hypernyms(self.get_synset(word)):
      synsets.add(hypernym1)
      for hypernym2 in self.get_hypernyms(hypernym1):
        synsets.add(hypernym2)
        for hypernym3 in self.get_hypernyms(hypernym2):
          synsets.add(hypernym3)

    synonyms = set()
    synonyms.add(word)
    # print(synonyms)
    for synset in synsets:
      synonyms.update(self.get_synset_word(synset))

    return synonyms

# print(get_synset('кот'))
# print(get_hypernyms('животное'))
# print(get_hyponyms('кот'))

# wikiwordnet = WikiWordnet()
# synsets = wikiwordnet.get_synsets('кот')
# synset1 = synsets[0]


# for hyponym in wikiwordnet.get_hyponyms(synset1):
#   print()
#   for w in hyponym.get_words():
#     # synonyms.append(w.lemma())
#     print(w.lemma())



# print()
# for hypernym in wikiwordnet.get_hypernyms(synset1):
#   for w in hypernym.get_words():
#     # synonyms.append(w.lemma())
#     print(w.lemma())

# for hyponym in wikiwordnet.get_hyponyms(synset1):
#   print({w.lemma() for w in hyponym.get_words()})
#   synsets = wikiwordnet.get_synsets(w.lemma())
#   synset1_1 = synsets[1]
#   for hypernym in wikiwordnet.get_hypernyms(synset1_1):
#     for w in hypernym.get_words():
#       print(w.lemma())

"""# Синонимы 4 Парсинг synonym.org"""

import time

class SynonymizerParser():
  def __init__(self):
    self.cache = {}
    self.sentence_preprocessor = SentencesPreprocessor()

  def get_synonyms_by_site(self, word):
    time.sleep(5)
    url = f'https://sinonim.org/s/{word}#list-s'
    headers = {
      'authority': 'sinonim.org',
      'method': 'GET',
      'path': '/s/%D0%BA%D0%BE%D1%82',
      'scheme': 'https',
      'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
      'accept-encoding': 'gzip, deflate, br',
      'accept-language': 'ru,en;q=0.9,nl;q=0.8',
      'cache-control': 'max-age=0',
      'cookie': '_ym_uid=1666721827903163901; _ym_d=1666721827; _ga=GA1.2.493696382.1677445558; _gid=GA1.2.1430881398.1678980046; _ym_isad=1; num_hits=12',
      'dnt': '1',
      'referer': 'https://sinonim.org/',
      'sec-ch-ua': '"Not?A_Brand";v="8", "Chromium";v="108", "Yandex";v="23"',
      'sec-ch-ua-mobile': '?0',
      'sec-ch-ua-platform': '"Windows"',
      'sec-fetch-dest': 'document',
      'sec-fetch-mode': 'navigate',
      'sec-fetch-site': 'same-origin',
      'sec-fetch-user': '?1',
      'upgrade-insecure-requests': '1',
      'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 YaBrowser/23.1.3.949 Yowser/2.5 Safari/537.36'
    }

    page = requests.post(url, headers=headers)
    soup = BeautifulSoup(page.text, "html.parser")

    synonyms = soup.findAll('a', href=True, id=True)
    synonyms = set(map(lambda syn: syn.text.strip(), synonyms))
    synonyms = set(filter(lambda syn: syn.count(' ') == 0, synonyms))
    synonyms.add(word)

    prep_synonyms = set()
    for syn in synonyms:
      prep_syn = self.sentence_preprocessor.normalize_sentence(syn)
      if prep_syn:
        prep_synonyms.add(prep_syn[0])

    return synonyms

  def get_synonyms(self, word):
    words = SentencesPreprocessor().normalize_sentence(word)
    if len(words) == 0:
      return set()

    word = words[0]
    return self.cache.get(word) if self.cache.get(word, None) else self.cache.setdefault(word, self.get_synonyms_by_site(word))

synonymizer_parser = SynonymizerParser()
synonymizer_parser.get_synonyms('стукнули')

"""# Pymystem3"""

# !pip install git+https://github.com/nlpub/pymystem3
# !wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz
# !tar -xvf mystem-3.0-linux3.1-64bit.tar.gz
# !cp mystem /root/.local/bin/mystem

# from pymystem3 import Mystem

# text = "Хорошая"
# m = Mystem()
# lemmas = m.lemmatize(text)
# print(''.join(lemmas))

"""# Стеммер"""


from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer(language="russian")
print(stemmer.stem("поругает"))
print(stemmer.stem("ругать"))
print(stemmer.stem("поругать"))

import Stemmer
stemmer = Stemmer.Stemmer('russian')
print(stemmer.stemWord("поругает"))
print(stemmer.stemWord("ругать"))
print(stemmer.stemWord("поругать"))

import pymorphy2
morph = pymorphy2.MorphAnalyzer()
morph.parse('поругаться')[0].normal_form

"""# Распределение по частям речи


"""

TRANSLATE_POS = {
    'ADJ': 'Прилагательное',
    'ADV': 'Наречие',
    'INTJ': 'Междометие',
    'NOUN': 'Существительное',
    'PROPN': 'Имя собственное',
    'VERB': 'Глагол',

    'ADP': 'Предикатив',
    'AUX': 'Вспомогательный глагол',
    'CCONJ': 'Сочинительный союз',
    'DET': 'Определитель',
    'NUM': 'Числительное',
    'PART': 'Частица',
    'PRON': 'Местоимение',
    'SCONJ': 'Подчиняющий союз',

    'PUNCT': 'Пунктуация',
    'SYM': 'Символ',
    'X': 'Другое'
}

def get_pos_distr(text) -> Dict:
  count = 0
  pos_dict = {
    'ADJ': 0,
    'ADV': 0,
    'INTJ': 0,
    'NOUN': 0,
    'PROPN': 0,
    'VERB': 0,

    'ADP': 0,
    'AUX': 0,
    'CCONJ': 0,
    'DET': 0,
    'NUM': 0,
    'PART': 0,
    'PRON': 0,
    'SCONJ': 0,

    'PUNCT': 0,
    'SYM': 0,
    'X': 0
}
  for token, pos in get_pos(text):
    # print(token, pos)
    if pos == 'PUNCT':
      continue

    pos_dict[pos] = pos_dict.setdefault(pos, 0) + 1

  return pos_dict

def get_pos_count(text, pos) -> int:
  return get_pos_distr(text).get(pos.upper(), 0)

def get_pos_count_percent(text, pos) -> float:
  count = len(tokenize_wrapper(text))
  return round(get_pos_distr(text).get(pos.upper(), 0) / count, 2)

#create pie chart
text = 'Птенца, там, мама птенцы, потом она улетела , и кошка пришла. Потом она стала залезать на дерево, потом прыгнула. Потом мама пришла у нее и спасла их. И собака зарычала на кошку. Потом собака за хвост дернула за кота. Потом выгнала кот собака.'
text2 = 'мальчик строил башенку, она у него получалась, но он поставил лишний кубик и она развалилась	потому что у него башенка развалилась	он поставил лишний кубик только что я сказала	будет дальше строить	кошка играла с цветком, кошка уронила цветок, упал цветок на собаку, собака разозлилась, нет, цветок упал на грабли и грабля дала кошке по носу	мама сказала девочке "сейчас мы будем пить чай", а девочка забыла что мама сказала, и побежала играть в игрушки, а потом когда девочку мама позвала.., девочка задела чашку'

pos_distr = get_pos_distr(text)
labels = [TRANSLATE_POS[pos] for pos, v in sorted(pos_distr.items(), key=lambda k: k[1]) if v != 0]
values = [pos_distr[k] for k, v in sorted(pos_distr.items(), key=lambda k: k[1]) if v != 0]

plt.figure(figsize=(6, 6))
plt.pie(values, labels=labels, autopct='%.0f%%')
plt.show()
labels

pos_distr = get_pos_distr(text2)
labels = [TRANSLATE_POS[pos] for pos, v in sorted(pos_distr.items(), key=lambda k: k[1], reverse=True) if v != 0]
values = [pos_distr[k] for k, v in sorted(pos_distr.items(), key=lambda k: k[1], reverse=True) if v != 0]

plt.figure(figsize=(6, 6))
plt.pie(values, labels=labels, autopct='%.0f%%')
plt.show()
labels

"""# Кореференция"""

# Инициализация:


# Инициализация моделей из Natasha:
g_segmenter = Segmenter()
emb = NewsEmbedding()
g_morph_tagger = NewsMorphTagger(emb)
g_syntax_parser = NewsSyntaxParser(emb)

"""## Синтаксический анализ"""

def build_tokens_list(a_text: str) -> Dict[str, natasha.doc.DocToken]:
    """
    Получение списка токенов из текста. Токены объединены в набор несвязных синтаксических деревьев,
    по одному на каждое предложение. Каждый токен хранит ссылку на своего предка в дереве.

    :param a_text: Входной текст.
    :return: Словарь токенов типа natasha.doc.DocToken. Ключами играют роль номера токенов
    DocToken#id, перевести которые в число невозможно. Ключи отсортированы по встрече
    соответствующих слов в тексте.
    """
    doc = Doc(a_text)
    doc.segment(g_segmenter)
    doc.tag_morph(g_morph_tagger)
    doc.parse_syntax(g_syntax_parser)

    # Составление словаря:
    map: Dict[str, natasha.doc.DocToken] = dict()
    for token in doc.tokens:
        map[token.id] = token

    return map


def draw_syntax_tree(a_tokens: Dict[str, natasha.doc.DocToken], a_drawer: Any, **kwargs) -> None:
    """
    Функция для отрисовки дерева. Принимает последним аргументом функцию, которая занимается
    отрисовкой.

    :param a_text: Список токенов типа natasha.doc.DocToken.
    :param a_drawer: Функция рисования.
    :param kwargs: Именованные аргументы для a_drawer.
    """
    new_id_to_old_id = {}
    counter: int = 0
    for token in a_tokens.values():
        if token.id not in new_id_to_old_id.keys():
            new_id_to_old_id[token.id] = counter
            counter += 1

    words, deps = [], []
    for token in a_tokens.values():
        words.append(token.text)
        source = new_id_to_old_id[token.head_id] if token.head_id in new_id_to_old_id.keys() else -1
        target = new_id_to_old_id[token.id]
        if source > 0 and source != target:
            deps.append([source, target, token.rel])

    a_drawer(words, deps, **kwargs)


def draw_ascii_syntax_tree(a_tokens: Dict[str, natasha.doc.DocToken], **kwargs) -> None:
    """
    Отрисовка синтаксического дерева в виде консольного вывода.

    См. draw_syntax_tree() выше.

    :param a_text: Список токенов типа natasha.doc.DocToken.
    :param kwargs: Именованные аргументы для a_drawer.
    """
    draw_syntax_tree(a_tokens, ipymarkup.show_dep_ascii_markup, **kwargs)


def draw_html_syntax_tree(a_tokens: Dict[str, natasha.doc.DocToken], **kwargs) -> None:
    """
    Отрисовка синтаксического дерева в виде html-картинки.

    :param a_text: Список токенов типа natasha.doc.DocToken.
    :param kwargs: Именованные аргументы для a_drawer.
    """
    draw_syntax_tree(a_tokens, ipymarkup.show_dep_markup, **kwargs)

"""## Поиск именных групп по синтаксическому дереву"""

class Referent:
    """
    Структура для референта. Содержит список именных групп, которые ссылаются на неё.
    """

    def __init__(self, a_first: NounPhrase):
        self.m_noun_phrases: List[NounPhrase] = [a_first]


    def __str__(self) -> str:
        return self.m_noun_phrases[0].head_text()


class NounPhrase:
    """
    Сткрутура для хранения информации по именной группе в необработанном виде, как есть.
    """

    def __init__(
        self,
        a_head: natasha.doc.DocToken,
        a_dom_noun_phrase: Optional[NounPhrase] = None,
        a_dom_verb_phrase: Optional[natasha.doc.DocToken] = None):

        self.m_head: natasha.doc.DocToken = a_head

        # Ближайшая доминирующая именная группа:
        self.m_dom_noun_phrase: Optional[NounPhrase] = a_dom_noun_phrase

        # Ближайшая доминирующая глагольная группа:
        self.m_dom_verb_phrase: Optional[natasha.doc.DocToken] = a_dom_verb_phrase

        # Крайне левый токен именной группы:
        self.m_far_left_word: natasha.doc.DocToken = a_head

        # Крайне правый токен именной группы:
        self.m_far_right_word: natasha.doc.DocToken = a_head

        # Ссылка на именную группу-перечисление, к которой, если, принадлежит данная именная группа:
        self.m_conjunct: Optional[NounPhrase] = None

        # Ссылка на референт:
        self.m_referent: Optional[Referent] = Referent(self)


    def __str__(self) -> str:
        return f'{self.m_head.id} {self.m_head.text}'
        # return f'{self.m_head.id} {self.m_head.text} ' \
        #        f'({self.m_far_left_word.id}:{self.m_far_right_word.id}) ' \
        #        f'{self.m_dom_noun_phrase.text} {self.m_dom_verb_phrase.text}'


    def head_text(self) -> str:
        """
        :return: Запись слова-вершины в виде строки.
        """
        return self.m_head.text


    def head_pos(self) -> str:
        """
        :return: Часть речи вершины.
        """
        return self.m_head.pos


    def head_feats(self) -> List[Dict[str, str]]:
        """
        :return: Грамматические признаки вершины.
        """
        return self.m_head.feats


    def head_rel(self) -> List[Dict[str, str]]:
        """
        :return: Синтаксическая роль вершины.
        """
        return self.m_head.rel


    def sentence_id(self) -> int:
        """
        :return: Номер предложения.
        """
        return int(self.m_head.id.split('_')[0])


    def head_id_in_sentence(self) -> int:
        """
        :return: Позиция вершины в предложении.
        """
        return int(self.m_head.id.split('_')[1])


def is_a_head_of_noun_phrase(a_token: natasha.doc.DocToken) -> bool:
    """
    Проверка, является однозначно ли токен a_token вершиной некой именной группы. Существуют именные
    группы, которые не будут проходить эту проверу: перечисления.

    :param a_token: Входной текст.
    :return: True или является, и False иначе.
    """
    return a_token.rel in ['nsubj', 'nsubj:pass', 'obl', 'iobj', 'obj', 'xcomp', 'appos'] and \
           a_token.pos not in ['PUNCT', 'X', 'ADP', 'ADJ', 'ADV' 'SCONJ', 'DET', 'VERB', 'NUM']


def is_a_head_of_verb_phrase(a_token: natasha.doc.DocToken) -> bool:
    """
    Проверка, является однозначно ли токен a_token вершиной некой глагольной группы. Существуют
    глагольные группы, которые не будут проходить эту проверу: перечисления.

    :param a_token: Входной текст.
    :return: True или является, и False иначе.
    """
    return a_token.rel == 'root' and a_token.pos is 'VERB'


def is_a_conjunct(a_token: natasha.doc.DocToken) -> bool:
    """
    Проверка, является ли токен a_token элементом последовательности перечислений, как правило
    разделённых запятой или союзами 'и' или 'или'.

    :param a_token: Входной текст.
    :return: True или является, и False иначе.
    """
    return a_token.rel == 'conj'


def get_all_noun_phrases(a_tokens: Dict[str, natasha.doc.DocToken]) -> Dict[str, NounPhrase]:
    """
    Поиск всех именных групп в синтаксическом дереве a_tokens при помощи системы правил.

    :param a_tokens: Токены входного текста, отсортированные по порядку встречи соответствующих
    слов.
    :return: Словарь объектов типа NounPhrase -- найденных именных групп, отсортированных по порядку
    встречи их вершин в тексте. Ключами выступают значения DocToken#id вершин соответствующих
    именных групп.
    """

    # Список найденных именных групп:
    res: Dict[str, NounPhrase] = dict()

    # Рекурсивная функция, преследующая цель обнаружить ближайшие доминирующие именную группу и
    # глагольную группу, а так же обновить для первой значения m_far_left_word и m_far_right_word.
    def rec_find_dom(a_token: natasha.doc.DocToken) -> \
        Tuple[Optional[NounPhrase], Optional[natasha.doc.DocToken]]:
        """

        :param a_token: Входной токен.
        :return: Пара из ближайших доминирующих именной и глагольной групп.
        """

        it: Optional[natasha.doc.DocToken] = a_tokens.get(a_token.id)
        while True:
            # Случай отсутствия очередного предка:
            if it is None:
                return None, None
            # Случай, когда очередной предок оказывается вершиной именной группы:
            elif is_a_head_of_noun_phrase(it):

                # Создать объект именной группы если необходимо:
                if it.id in res.keys():
                    np = res[it.id]
                else:
                    try:
                        np = NounPhrase(it, *rec_find_dom(a_tokens.get(it.head_id)))
                        res[it.id] = np
                    except RecursionError:
                        print('rec!')
                        return None, it

                # Обновить m_far_left_word для np:
                if a_token.start < np.m_far_left_word.start:
                    np.m_far_left_word = a_token

                # Обновить m_far_right_word для np:
                if np.m_far_right_word.start < a_token.start:
                    np.m_far_right_word = a_token

                # Обновить m_conjunct для np:
                if is_a_conjunct(a_token):

                    # Создать объект именной группы для a_token:
                    old_np = NounPhrase(a_token)
                    res[a_token.id] = old_np

                    # Создать объект именной группы-перечисления если необходимо:
                    if np.m_conjunct is None:
                        conjunct_np = NounPhrase(it, np.m_dom_noun_phrase, np.m_dom_verb_phrase)
                        # Установить для np доминирующей именной группой conjunct_np:
                        np.m_dom_noun_phrase = conjunct_np
                        res[it.id + '_conjunct'] = conjunct_np
                    else:
                        conjunct_np = np.m_conjunct

                    # Установить и для old_np то же самое что и для np:
                    old_np.m_dom_noun_phrase = conjunct_np
                    old_np.m_dom_verb_phrase = np.m_dom_verb_phrase

                    res[a_token.id].m_conjunct = conjunct_np

                return np, np.m_dom_verb_phrase
            # Случай, когда очередной предок оказывается вершиной глагольной группы:
            elif is_a_head_of_verb_phrase(it):
                return None, it

            # Установить итератор на предка текущего токена:
            it = a_tokens.get(it.head_id)

    # Переберём все токены и проверим, являются ли они вершиной именной группы:
    for token in a_tokens.values():
        rec_find_dom(token)

    return res


def draw_noun_phrases_in_text(
    a_text: str, a_noun_phrases: Dict[str, NounPhrase], a_drawer: Any, **kwargs) -> None:
    """
    Нарисовать текст с обозначенными именными группами. Принимает последним аргументом функцию,
    которая занимается отрисовкой.

    :param a_text: Исходный текст.
    :param a_noun_phrases: Словарь объектов типа NounPhrase, отсортированных по порядку встречи их
    вершин в тексте. Ключами выступают значения DocToken#id вершин соответствующих именных групп.
    :param a_drawer: Функция рисования.
    :param kwargs: Именованные аргументы для a_drawer.
    """

    # Составить список отрезков:
    lines: List[Tuple[int, int, str]] = []
    for np in a_noun_phrases.values():
        if np.m_referent is None:
            lines.append((np.m_far_left_word.start, np.m_far_right_word.stop,
                          np.head_text()))
        else:
            lines.append((np.m_far_left_word.start, np.m_far_right_word.stop,
                          np.m_referent.m_noun_phrases[-1].head_text()))

    a_drawer(a_text, lines, **kwargs)


def draw_ascii_noun_phrases_in_text(
    a_text: str, a_noun_phrases: Dict[str, NounPhrase], **kwargs) -> None:
    """
    Нарисовать текст с обозначенными именными группами в виде консольного вывода.

    :param a_text: Исходный текст.
    :param a_noun_phrases: Словарь объектов типа NounPhrase, отсортированных по порядку встречи их
    вершин в тексте. Ключами выступают значения DocToken#id вершин соответствующих именных групп.
    :param kwargs: Именованные аргументы для a_drawer.
    """
    draw_noun_phrases_in_text(a_text, a_noun_phrases, ipymarkup.show_span_ascii_markup, **kwargs)


def draw_html_noun_phrases_in_text(
    a_text: str, a_noun_phrases: Dict[str, NounPhrase], **kwargs) -> None:
    """
    Нарисовать текст с обозначенными именными группами в виде html-картинки.

    :param a_text: Исходный текст.
    :param a_noun_phrases: Словарь объектов типа NounPhrase, отсортированных по порядку встречи их
    вершин в тексте. Ключами выступают значения DocToken#id вершин соответствующих именных групп.
    :param kwargs: Именованные аргументы для a_drawer.
    """
    draw_noun_phrases_in_text(a_text, a_noun_phrases, ipymarkup.show_span_line_markup, **kwargs)

"""## Разрешение большинства случаев кореференции системой правил"""

# # Возможные результаты условий:
# class enum_condition_result(Enum):
#     unknown = 0
#     true = 1
#     false = 2

# # Список условий:
# def condition_1(a_left: NounPhrase, a_right: NounPhrase) -> enum_condition_result:
#     """
#     Между достаточно удалёнными друг от друга именными группами не может быть кореферентной связи.

#     :param a_left: Более ранняя именная группа.
#     :param a_right: Более поздняя именная группа.
#     :return: Значение типа enum_condition_result: true если кореференция точно присутствует, false
#     если кореференция точно отсутствует, и unknown если возможны оба варианта.
#     """

#     if abs(a_left.sentence_id() - a_right.sentence_id()) > 4:
#        return enum_condition_result.false

#     return enum_condition_result.unknown


# def condition_2(a_left: NounPhrase, a_right: NounPhrase) -> enum_condition_result:
#     """
#     Разрешение кореференции на основе грамматических признаков двух именных групп, одной из которых
#     является местоимением.

#     :param a_left: Более ранняя именная группа.
#     :param a_right: Более поздняя именная группа.
#     :return: Значение типа enum_condition_result: true если кореференция точно присутствует, false
#     если кореференция точно отсутствует, и unknown если возможны оба варианта.
#     """

#     left_feats: natasha.doc.DocToken = a_left.head_feats()
#     right_feats: natasha.doc.DocToken = a_right.head_feats()

#     left_pos: natasha.doc.DocToken = a_left.head_pos()
#     right_pos: natasha.doc.DocToken = a_right.head_pos()

#     left_is_pron: bool = left_pos == 'PRON'
#     right_is_pron: bool = right_pos == 'PRON'

#     # Является ли одна из именных групп местоимением:
#     if not (left_is_pron or right_is_pron):
#         return enum_condition_result.unknown

#     # Если число у местоимения -- множественное, то определить кореференцию здесь нельзя:
#     if 'Number' in left_feats and 'Number' in right_feats:
#         if (left_is_pron and left_feats['Number'] in ['Plur', 'Ptan', 'Coll']) or \
#         (right_is_pron and right_feats['Number'] in ['Plur', 'Ptan', 'Coll']):
#             return enum_condition_result.unknown

#         # Если число -- единственное:
#         elif left_feats['Number'] == right_feats['Number'] == 'Sing':

#             # Если обе именных группы обладают одинаковым грамматическим родом -- однозначно
#             # кореференция:
#             if 'Gender' in left_feats and 'Gender' in right_feats:
#                 if left_feats['Gender'] == right_feats['Gender']:
#                     return enum_condition_result.unknown

#     # Во всех остальных случаях, связанных с местоимением:
#     return enum_condition_result.false


# def condition_3(a_left: NounPhrase, a_right: NounPhrase) -> enum_condition_result:
#     """
#     Между агенсами и пациенсами одного предложения не может быть кореферентной связи.

#     :param a_left: Более ранняя именная группа.
#     :param a_right: Более поздняя именная группа.
#     :return: Значение типа enum_condition_result: true если кореференция точно присутствует, false
#     если кореференция точно отсутствует, и unknown если возможны оба варианта.
#     """

#     left_is_core_arg: bool = a_left.m_dom_noun_phrase is None
#     right_is_core_arg: bool = a_right.m_dom_noun_phrase is None

#     if a_left.sentence_id() == a_right.sentence_id() and left_is_core_arg and right_is_core_arg:
#        return enum_condition_result.false

#     return enum_condition_result.unknown


# def condition_4(a_left: NounPhrase, a_right: NounPhrase) -> enum_condition_result:
#     """
#     Между местоимением и однородным членом не может быть кореферентной связи.

#     :param a_left: Более ранняя именная группа.
#     :param a_right: Более поздняя именная группа.
#     :return: Значение типа enum_condition_result: true если кореференция точно присутствует, false
#     если кореференция точно отсутствует, и unknown если возможны оба варианта.
#     """

#     if (a_left.head_pos == 'PORN' and a_right.m_conjunct is not None) or \
#        (a_right.head_pos == 'PORN' and a_left.m_conjunct is not None):
#        return enum_condition_result.false

#     return enum_condition_result.unknown

# def stage_1_get_distributions(
#     a_tokens: Dict[str, natasha.doc.DocToken],
#     a_noun_phrases: Dict[str, NounPhrase]) \
# -> Dict[NounPhrase, NounPhrase]:
#     """
#     Процедура отсева только тех пар именных групп, которые не противоречат условиям выше.

#     :param a_tokens: Токены входного текста, отсортированные по порядку встречи соответствующих
#     слов.
#     :param a_noun_phrases: Словарь объектов типа NounPhrase -- найденных именных групп,
#     отсортированных по порядку встречи их вершин в тексте. Ключами выступают значения DocToken#id
#     вершин соответствующих именных групп.
#     :return: Множество пар именных групп, проходящих систему правил.
#     """

#     # Варианты для каждой именной группы:
#     distributions: Dict[NounPhrase, Set[NounPhrase]] = dict()

#     # Цикл по всем (не отсортированным: (a, b) = (b, a)) парам (без повторов) из a_noun_phrases:
#     for pair in combinations(a_noun_phrases.values(), 2):

#         # Список проверок (список указателей на функции):
#         conditions_list: List[Any] = [
#             condition_1,
#             condition_2,
#             condition_3,
#             condition_4
#         ]

#         # Провести последовательно через все проверки:
#         for condition in conditions_list:

#             # Применение очередного условия:
#             res: enum_condition_result = condition(*pair)

#             # Если кореференции однозначно нет -- прервать цикл и никуда не добавлять:
#             if res == enum_condition_result.false:
#                 break

#             # Если кореференция точно есть -- декларативно оставить pair[0] единственным вариантом
#             # для pair[1]:
#             elif res == enum_condition_result.true:
#                 distributions[pair[1]] = [pair[0]]
#                 break

#         # Если цикл не был прерван, то добавить пару в distributions:
#         else:
#             if pair[1] not in distributions:
#                 distributions[pair[1]] = set()
#             distributions[pair[1]].add(pair[0])

#     return distributions


# def make_coreferent(a_left: NounPhrase, a_right: NounPhrase) -> None:
#     """
#     Произвести необходимые в данной программе действия, что бы сделать две именные группы
#     кореферентными.

#     :param a_left: Более ранняя именная группа.
#     :param a_right: Более поздняя именная группа.
#     :return: Референт.
#     """

#     ref: Referent = a_left.m_referent
#     ref.m_noun_phrases.extend(a_right.m_referent.m_noun_phrases)
#     a_right.m_referent = ref
#     return ref


# def stage_1(
#     a_tokens: Dict[str, natasha.doc.DocToken],
#     a_noun_phrases: Dict[str, NounPhrase]) \
# -> Tuple[Dict[NounPhrase, NounPhrase], Set[Referent]]:
#     """
#     Система правил, которая автоматически решает одни, и отсевивает другие, пары именных групп.

#     :param a_tokens: Токены входного текста, отсортированные по порядку встречи соответствующих
#     слов.
#     :param a_noun_phrases: Словарь объектов типа NounPhrase -- найденных именных групп,
#     отсортированных по порядку встречи их вершин в тексте. Ключами выступают значения DocToken#id
#     вершин соответствующих именных групп.
#     :return: Возвращает список пар, задачу кореференции для которых должна решить вторая стадия,
#     и множество референтов с хотя бы двумя референтными к нему именными группами.
#     """

#     referents: Set[Referent] = set()

#     # Получение возможных кореферентных пар:
#     distributions: Dict[NounPhrase, NounPhrase] = \
#                 stage_1_get_distributions(a_tokens, a_noun_phrases)

#     # Пройтись по тем распределениям в distributions, в которых присутствует только один вариант:
#     for_deletion: List[NounPhrase] = []
#     for np in distributions:
#         if len(distributions[np]) == 1:
#             for_deletion.append(np)
#             make_coreferent(np, distributions[np].pop())
#             referents.add(np.m_referent)

#     # Удалить теперь лишние распределения из distributions:
#     for np in for_deletion:
#         del distributions[np]

#     return distributions, referents

"""## Примеры"""

# # Пример:
# example_text = "Гражданин отвечает по своим обязательствам всем принадлежащим ему имуществом."

# # Преобразуем текст в набор токенов:
# example_tokens = build_tokens_list(example_text)
# display(example_tokens)
# print()

# # Получаем:
# example_noun_phrases = get_all_noun_phrases(example_tokens)
# example_distributions = stage_1(example_tokens, example_noun_phrases)[0]


# draw_html_noun_phrases_in_text(example_text, example_noun_phrases,
#                                label_size=12, background='black')
# print()
# for np in example_distributions:
#     print(np, ':', *example_distributions[np])

# Пример:
example_text = 'Жила-была птичка. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать.'

# Преобразуем текст в набор токенов:
example_tokens = build_tokens_list(example_text)
#display(example_tokens)
print(example_tokens)

# Получаем:
# example_noun_phrases = get_all_noun_phrases(example_tokens)
# example_distributions = stage_1(example_tokens, example_noun_phrases)[0]


# draw_html_noun_phrases_in_text(example_text, example_noun_phrases,
#                                label_size=12, background='black')
# print()
# for np in example_distributions:
#     print(np, ':', *example_distributions[np])

"""# Синтаксис предложения"""

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '
text = 'Мальчик разбил, а потом на девочку наговаривает.'
syntax_parser = NewsSyntaxParser(emb)

doc = Doc(text)
doc.segment(segmenter)
doc.parse_syntax(syntax_parser)
print(doc.tokens)
doc.sents[0].syntax.print()

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '
text = 'и палкой кошке прямо в нос,     '
syntax_parser = NewsSyntaxParser(emb)

doc = Doc(text)
doc.segment(segmenter)
doc.parse_syntax(syntax_parser)
print(doc.tokens)
doc.sents[0].syntax.print()

syntax_parser = NewsSyntaxParser(emb)

class SyntaxTree:
  def __init__(self, text):
    self.doc = Doc(text)
    self.doc.segment(segmenter)
    self.doc.parse_syntax(syntax_parser)

    self.count_sents = len(self.doc.sents)
    print(self.doc.text)

  def get_roots_by_sent(self, n_sent):
    roots = []
    for token in self.doc.sents[n_sent].syntax.tokens:
      if token.rel == 'root':
        roots.append((token.id, token.text))

    return roots

  def print_tree(self, n_sent: int):
    self.doc.sents[n_sent].syntax.print()

  def get_num(self, id: str):
    return int(id[id.index('_')+1:])

  def get_ids_by_text(self, text: str, n_sent: int):
    ids = []
    for token in self.doc.sents[n_sent].syntax.tokens:
      if token.text == text:
        ids.append(self.get_num(token.id))
    return ids

  def token_parents(self, token_text: str, n_sent: int):
    # TODO переделать на id
    parents = []
    for token in self.doc.sents[n_sent].syntax.tokens:
      if token.text == token_text and token.rel != 'root':
        parents.append(self.doc.sents[n_sent].syntax.tokens[self.get_num(token.head_id)])
    return parents

  def token_children(self, token_id: str, n_sent: int):
    children = []
    for child_token in self.doc.sents[n_sent].syntax.tokens:
      if child_token.head_id == token_id and child_token.rel != 'punct':
        children.append((child_token.id, child_token.text))
    return children

  def get_rels(self, depth: int):
    def rec_get_rels(rels: List, word_id: str, n_sent: int, prev_words: List):
      children = self.token_children(word_id, n_sent)
      if children:
        for child_id, child_text in children:
          if len(prev_words) == depth-1:
            rels.append(prev_words + [child_text.lower()])
            rec_get_rels(rels, child_id, n_sent, prev_words[1:] + [child_text.lower()])
          else:
            rec_get_rels(rels, child_id, n_sent, prev_words + [child_text.lower()] )

    rels = []

    if depth == 1:
      for token in self.doc.tokens:
        if token.rel != 'punct':
          rels.append([token.text.lower()])
    else:
      for n_sent in range(self.count_sents):
        for root_id, root_text in self.get_roots_by_sent(n_sent):
          rec_get_rels(rels, root_id, n_sent, [root_text.lower()])

    return rels

  def test(self):
    for n_sent in range(self.count_sents):
      print(tree.doc.sents[n_sent])

text = 'Птица.. Дети  птицы хотели кушать. Мама полетела, а кот это увидел и  полез за ними, схватил одного, он запищал. Мама улетела, и вот пришла собака, и его за хвост  укусила, его за хвост. И он убежал отсюдова, а собака за котом потом убежал.'
# text = 'Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой.'
text = 'Однажды на дереве птица свила гнездо и кормит там птенцов.'
tree = SyntaxTree(text)
tree.get_rels(2)

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '
# text = 'Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой.'
tree = SyntaxTree(text)
print(tree.doc.tokens)
print(tree.count_sents)
tree.get_roots_by_sent(2)

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '
tree = SyntaxTree(text)
tree.print_tree(10)
# print(tree.headers)

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '

syntax_parser = NewsSyntaxParser(emb)

doc = Doc(text)
doc.segment(segmenter)
doc.parse_syntax(syntax_parser)
print(doc.tokens[:5])
doc.sents[0].syntax.print()

"""# Обработка датасета дошкольников

"""

path_data_speech = 'Тексты детей по РФ и эмоциям_parsed.csv'
df_new = pd.read_csv(path_data_speech)
df_new.fillna('', inplace=True)
df_new

df1 = df_new[['order1', 'time1', 'is_strong']]
df1['tell1'] = df_new['tell1'].map(lambda s: re.sub('\.{2,} ?', ' ', s.replace('\t', ' ')))
df1 = df1[df1['tell1'] != '']
df1

df2 = df_new[['order2', 'time2', 'is_strong']]
df2['tell2'] = df_new['tell2_prep'].map(lambda s: re.sub('\.{2,} ?', ' ', s.replace('\t', ' ')))
df2 = df2[df2['tell2'] != '']
df2

df3 = df_new[['time3', 'is_strong']]
df3['tell3'] = df_new['tell3_prep'].map(lambda s: re.sub('\.{2,} ?', ' ', s.replace('\t', ' ')))
df3 = df3[df3['tell3'] != '']
df3

"""# Параметры Ощепкова

Статистика:
  из 21 параметра:

    - 5 сделано

    - 1 невозможно сделать на данный момент
    
    - 15 ???

Итого: сделано ≈ 19%
"""

df.fillna(value={'answer': ''}, inplace=True)
ex = df.loc[193]
ex

ex.tell

print(' '.join(preprocess(ex.tell)))

"""## 1. Смысловая полнота

Смысловая полнота рассказа, которая оценивается суммой баллов, начисляемой за упоминание ключевых моментов.

3 балла ставятся, если ребенок  самостоятельно упоминает их в рассказе;

1,5 балла — если называет после вопросов психолога, или при нeтoчнoм опрeдeлeнии объeкта или дeйствия,

0 баллов — отсутствиe упоминания.
"""

text = 'Человек должен стремиться к высшей цели. Жила-была птичка. У нее. Она жила на югу в дубу. И у нее были птенцы. Однажды птенцы захотели кушать. Их мама птичка улетела за едой, а котик увидел птичек, которые остались одни, ну птенцов, и захотел их скушать. А когда мама улетела, потом она вернулась с ягодками, а котик уже начал взбираться на дерево. Потом котик забрался к гнезду и тут прибежала собака. Собака увидела, что котик хочет съесть птенцов и погналась за кошкой. Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой. '
# text = 'Собака схватила за хвост кошку, а мама в то время видела, что кошка хотела съесть её птенцов, а собака вытащила кошку с дерева и погналась за кошкой.'

tree = SyntaxTree(text)
print(tree.count_sents)
tree.get_rels(1)

SentencesPreprocessor().normalize_sentence('поругать')

synonymizer = synonymizer_parser
synonymizer.get_synonyms('башня')

synonymizer.get_synonyms('гнездо')

DEFAULT_PREFIXES = ('а','агит','ан','англо','анти','атто','без','бес','брам','в','вз','вне','военно','воз','вос','вы','гекса','гексаконта','гекта','гекто','гепта','гептаконта','гига',
'гипер','гор','гос','де','дез','дека','деци','дикта','до','додека','за','зепто','зетта','из','изо','ин','интервики','интра','инфра','йокто','йотта','квадра','квази','кила','кило',
'ко','кое','контр','лейб','мега','меж','микро','милли','мини','мириа','моно','на','над','наи','нано','не','недо','ни','низ','нис','нона','о','около','окта','октаконта','от','па','пара',
'пентаконта','пере','пета','пико','по','под','после','пост','пра','пре','пред','при','про','прото','раз','ре','роз','рос','с','санти','сверх','со','су','суб','супер','супра','сюр','тера',
'тетра','тетраконта','транс','тре','три','триаконта','тридека','трикта','у','ультра','ундека','фемто','черес','эйкоза','экзо','экс','экса','экстра','эннеаконта')

def calc_semantic_completeness(text, semantic_components):
  # semantic_components = ['птица', 'мама', 'птенцы', 'кот', 'схватить', 'кормит', 'отвернулась', 'собака', 'схватила', 'спасла', 'прогнала'] # 0.73
  # semantic_components = ['девочка', 'мама', 'мальчик', 'чашки', 'стол', 'осколки', 'ругать', 'девочку', 'прячется', 'блюдце']
  # «Гнездо» 1 птица-мама;  2 птенцы;  3 котик;  4 хочет схватить птенца;  5 кормит птенцов;  6 отвернулась;  7 собака;  8 схватила кота;  9 спасла птенцов;  10 прогнала кота)
  def compare_lists(l1, l2):
    if len(l1) != len(l2):
      raise Exception(f'Длина списков не совпадает: {len(l1)} != {len(l2)}')

    for i in range(len(l1)):
      if l1[i] == l2[i]:
        continue
      w1, w2 = (l1[i], l2[i]) if len(l1[i]) < len(l2[i]) else (l2[i], l1[i])

      if w2 not in list(map(lambda prefix: prefix + w1, DEFAULT_PREFIXES)):
        return False

    return True

  def get_comp_synonyms(comps):
    if type(comps) is not list:
      comps = [comps]
    synonymizer = synonymizer_parser

    comp_synonyms = []
    for comp in comps:
      for comp in product(*[token.split('/') for token in comp.split(' ')]):
        comp_synonyms_list = [synonymizer.get_synonyms(comp_word) for comp_word in comp]

        # print(comp_synonyms_list)
        for comp_synonym in product(*comp_synonyms_list):
          comp_synonyms.append(list(comp_synonym))

    return comp_synonyms

  sent_preprocess = SentencesPreprocessor()
  norm_text = sent_preprocess.normalize_sentence(text)
  # text = text.replace('.', ',')

  count = 0

  tree = SyntaxTree(text)
  rels_dict = {}
  for i in range(1, 4):
    rels_dict[i] = []
    for rel in tree.get_rels(i):
      prep_rel = sent_preprocess.normalize_iter(rel)
      if len(prep_rel) == i:
        rels_dict[i].append(prep_rel)

  print(rels_dict[2])
  for comp in semantic_components:
    find_flag = False
    for prep_comp_synonym in get_comp_synonyms(comp):
      # print(f'prep_comp_synonym = {prep_comp_synonym}')
      for prep_rel in rels_dict.get(len(prep_comp_synonym), []):
        # prep_rel = sent_preprocess.normalize_iter(rel)
        # print(f'--{prep_rel} ||| {prep_comp_synonym}')
        # stem_prep_rel = list(map(stemmer.stem, prep_rel))
        # stem_prep_comp_synonym = list(map(stemmer.stem, prep_comp_synonym))
        # print(f'--{stem_prep_rel} ||| {stem_prep_comp_synonym}')

        if len(prep_comp_synonym) and len(prep_rel) and compare_lists(prep_rel, prep_comp_synonym):
          print(f'--{prep_rel} ||| {prep_comp_synonym}')
          count += 1
          find_flag = True
          break
      if find_flag:
        break

  return round(count/len(semantic_components), 2)

calc_semantic_completeness('Однажды на дереве птица свила гнездо и кормит там птенцов. ', ['птица', 'птенцы', 'котик', 'схватить птенца', 'свить гнездо', 'кормит птенцов', 'отвернулась', 'собака', 'схватила кота',
                         'спасла/кормит птенцов', ['прогнала кота', 'кормит птенцов']])

calc_semantic_completeness('мальчик строил пирамидку, достроил. Потом она у него упала и он заплакал. Потом он решил еще раз построить другую пирамидку, но уже с округлением.',
                           ['строил/делал мальчик/ребёнок', 'строил/делал башня/пирамидка', 'разрушилась/сломалась/упала башня/пирамидка/она'])

"""## 2. Смысловая адекватность А и Б

0 баллов ставится, если в рассказе верно передан смысл происходящего.

Смысловая адекватность оценивается два раза: по наличию ошибок левополушарного и правополушарного типа.

Смыслoвая неадекватнoсть из-за слабoсти левoпoлушарных функций oценивается oт oднoгo дo трех баллoв.

1 балл ставится за пропуск одного из ключевых смысловых звеньев, самостоятельно корригируемый.

2 балла – при пропуске 1-2 ключевых смысловых звеньев.

3 балла подразумевает пропуск трех и более смысловых звеньев, сохраняющийся после наводящих вопросов.

Смысловая неадекватность из-за слабости правополушарных функций также может быть оценена от одного до трех баллов.

1 балл начисляется за нарушение связи между событиями при верном их описании.

2 балла – при ошибочном толковании событий с ошибочным восприятием или игнорированием 1-2 компонентов картинок.

При нереалистическом толковании событий с игнорированием нескольких компонентов картинок ставятся 3 балла.

## 3. Прогрaммировaние рaсскaзa.

0 бaллов – рaсскaз включaет основные смысловые единицы в прaвильной последовaтельности и с нaличием связующих звеньев.

1 балл – пропуск отдельных смысловых единиц, отсутствие связующих звеньев или наличие необоснованных повторов связующих слов.

2 балла – пропуск смысловых звеньев, тенденция к перечислению деталей, наличие смысловых повторов и разрывов в повествовании.

3 балла – отсутствие  связного текста.

## 4. Грамматическое оформление рассказа

От 0 баллов за грамматически правильно оформленный рассказ с использованием разных грамматических конструкций;

до 3 баллов, если допускаются множественные синтаксические ошибки, в том числе пропуск глагольного сказуемого.

## 5. Количество параграмматизмов

(Вики) Парагматизм - это нарушение разговорной речи в результате повреждения различных областей мозга, вызывающее то, что известно как афазия (языковые расстройства). Прагматизм можно разделить на беглые афазии, то есть те, при которых затронутые люди могут выражать себя без усилий, с небольшими нарушениями артикуляции, но с отсутствием содержания, словарного запаса речи и т. Д.

Люди, пострадавшие от прагматизм как правило дезорганизовывать предложения, заменяя правильные формы неправильными грамматическими формами.

Нaпpимеp, "Галка пepеoдeлась 6eлым цветoм" - веpбaльнaя зaменa "пеpеoделась" вместo "пoкpаcилaсь" влечет зa сoбoй oшибкy yпpaвления

## 6. Длина рассказа (количество слов и предложений)  (DONE)

Количество слов в самостоятельном рассказе, а также количество в рассказе, включая ответы на вопросы.
"""

print(f'Количество слов в тексте: {len(tokenize_wrapper(ex.tell))}')
print(f'Количество предложений в тексте: {len(sentenize_wrapper(ex.tell))}')
print(f'Количество уникальных слов в тексте: {len(get_word_uniq(ex.tell))}\n')

print(f'Количество слов в тексте, включая ответы на вопросы: {len(tokenize_wrapper(ex.tell + ex.answer))}')
print(f'Количество предложений в тексте, включая ответы на вопросы: {len(sentenize_wrapper(ex.tell + ex.answer))}')
print(f'Количество уникальных слов в тексте, включая ответы на вопросы: {len(get_word_uniq(ex.tell + ex.answer))}')

"""## 7. Количество сложных предложений (DONE)

"""

def count_compound_sentences(tell):
  count_sentences = len(sentenize_wrapper(tell))
  count_compound_sentences = len([sent for sent in sentenize_wrapper(tell) if sent.count(',') > 0])
  return count_compound_sentences

def count_compound_sentences_percent(tell):
  count_sentences = len(sentenize_wrapper(tell))
  count_compound_sentences = len([sent for sent in sentenize_wrapper(tell) if sent.count(',') > 0])
  return round(count_compound_sentences/count_sentences, 2)

print(f'Количество сложных предложений: {count_compound_sentences(ex.tell)}')
print(f'Процент сложных предложений: {count_compound_sentences_percent(ex.tell)}')

"""## 8. Средняя длина фразы (DONE)

"""

def mean_sentences_len(tell):
  sentences_len = list(map(lambda x: len(tokenize_wrapper(x)), sentenize_wrapper(tell)))
  return np.mean(sentences_len)

sentences_len = list(map(lambda x: len(tokenize_wrapper(x)), sentenize_wrapper(ex.tell)))
get_statisctics(sentences_len, text = 'количество слов в предложении')
print('\n')

sentences_len = list(map(lambda x: len(preprocess(x)), sentenize_wrapper(ex.tell)))
get_statisctics(sentences_len, text = 'количество уникальных слов в предложении')

"""## 9. Правильные ответы на вопросы (???)

## 10. Лексическое оформление рассказа.

0 баллов ставится при адекватном использовании лексических средств ребенком.

1 – при поиске слов или единичных близких вербальных заменах.

2 балла подразумевают бедность словаря, вербальные парафазии.

3 – это выраженная бедность словаря, множественные лексические ошибки.

## 11. Разнообразие словаря (TTR) (DONE)

Лексическое разнообразие словаря (Type-Token Ratio в англоязычной традиции);

TTR = количество уникальных лексем, или лемм, в анализируемом тексте /  количество текстоформ (общее количество словоформ) в анализируемом тексте
"""

def ttr(tell):
  return round(len(get_word_uniq(tell))/len(preprocess(tell)), 2)

print(f'TTR = {ttr(ex.tell)}')

"""## 12. Прономинализация (DONE)

Показатель прономинализации - количество местоимений и их удельный вес в текстах;

"""

print(f'Количество местоимений - {get_pos_count(ex.tell, "PRON")}')
print(f'Удельный вес местоимений - {get_pos_count_percent(ex.tell, "PRON")}')

"""## 13. Количество атрибутов (прилагательных и наречий) (DONE)

"""

print(f'Количество прилагательных - {get_pos_count(ex.tell, "ADJ")}')
print(f'Удельный вес прилагательных - {get_pos_count_percent(ex.tell, "ADJ")}\n')

print(f'Количество наречий - {get_pos_count(ex.tell, "ADV")}')
print(f'Удельный вес наречий - {get_pos_count_percent(ex.tell, "ADV")}')

"""## 14. Неузуальная инверсия

Необычный инвертированный порядок слов

## 15. Составные глагольные сказуемые

## 16. Незаполненные валентности

Количество незаполненных валентностей и их частота;

https://ru.wikipedia.org/wiki/Валентность_(лингвистика)

## 17. Страдательный залог (+ возвратные глаголы)

Возвратные глаголы и единичные случаи употребления страдательного залога;

## 18. Логические коннекторы

Логические коннекторы (союзы, вводные слова, наречия и другие части речи, выражающие разного рода логические отношения в тексте);

## 19. Критерий «Ситуативность»

## 20. Критерий «Причинность»

## 21. Соблюдение нарративной структуры «Цель – попытка – результат»

Указание на цель. Мы засчитывали этот параметр, если ребенок указывал, зачем или куда идет агенс: *Мужик шёл, чтобы выбросить мусор;* *Дедушка хотел выбросить мусор* и т.п.

Указание на попытку. Мы засчитывали этот параметр, если ребенок указывал, что именно делал агенс, чтобы достичь своей цели: «Пошёл, прицелился, выкинул» или «Пошёл он выкидывать мусор».

Указание на результат:
*   выбросил – не выбросил мусор  
*   испачкался

## 22. Тип нарратива: полный / сокращенный / искаженный

*   полный («псевдостандартный») нарратив (complete)
*   упрощенный нарратив (simplification of the narrative field)
*   искаженный нарратив (distortion of the narrative line)

# Кластеризация

## Датасет дошкольников
"""

def create_attrs(tell, semantic_components):
  df = pd.concat([tell, tell.apply(lambda s: len(tokenize_wrapper(s)))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: len(sentenize_wrapper(s)))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: len(get_word_uniq(s)))], axis=1)
  df = pd.concat([df, tell.apply(count_compound_sentences_percent)], axis=1)
  df = pd.concat([df, tell.apply(mean_sentences_len)], axis=1)
  df = pd.concat([df, tell.apply(ttr)], axis=1)
  df = pd.concat([df, tell.apply(lambda s: get_pos_count_percent(s, "ADJ"))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: get_pos_count_percent(s, "ADV"))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: get_pos_count_percent(s, "PRON"))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: get_pos_count_percent(s, "NOUN"))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: get_pos_count_percent(s, "VERB"))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: calc_semantic_completeness(s, semantic_components[0]))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: calc_semantic_completeness(s, semantic_components[1]))], axis=1)
  df = pd.concat([df, tell.apply(lambda s: calc_semantic_completeness(s, semantic_components[2]))], axis=1)

  df.columns = [tell.name] + ['Кол-во слов', 'Кол-во предл', 'Кол-во уник. слов', 'Кол-во слож предл', 'Средняя длина предл',
                'TTR', 'Проц прил', 'Проц наречий', 'Проц мест', 'Проц сущ', 'Проц глаг', 'Семант полнота',
                'Сущ сем комп', 'Действия сем комп']

  return df

def get_df(df, postfix, semantic_components):
  df_cor = create_attrs(df[f'tell{postfix}'], semantic_components).iloc[:, 1:]
  df_cor['time'] = df[f'time{postfix}']
  df_cor['is_strong'] = df['is_strong']
  return df_cor

def get_clusters(df_cor):
  scaler = preprocessing.StandardScaler().fit(df_cor)
  df_scaled = scaler.transform(df_cor)

  clustering = KMeans(n_clusters=2, random_state=0).fit_predict(df_scaled)
  # clustering = SpectralClustering(n_clusters=2, n_neighbors=3, n_init=2, affinity='nearest_neighbors').fit_predict(df_scaled)

  return clustering

def get_heatmap(df_cor):
  plt.figure(figsize=(10,5))
  sns.heatmap(df_cor.corr(), annot = True, vmin=-1, vmax=1, center= 0)

"""1) кошка, киска, котенок
2) сидит на окне (на подоконнике)
3) увидел/а собаку
4) хочет навредить
5) сбросил цветочный горшок
6) рядом стояли грабли
7) цветок упал на грабли
8) из-за этого (или подразумеваемая связь)
9) грабли стукнули котенка
10) собака не пострадала
"""

Semantic_components = namedtuple('Semantic_components', ['full' , 'entities', 'actions'])

semantic_components_full = ['кошка', 'собака', 'навредить', 'сбросил/уронил цветок/горшок', 'упал грабли', ['грабли стукнули/кошке/дала', 'стукнули палка', 'кошке палка']]
semantic_components_entities = ['кошка', 'собака', 'цветок/горшок', 'грабли/палка/щетка']
semantic_components_actions = ['уронить/сбросить', 'упал', 'стукнуть/дала']
semantic_components2 = Semantic_components(semantic_components_full, semantic_components_entities, semantic_components_actions)

semantic_components_full = ['ругает мама/тетя', 'поругать/ругает девочка/дочка', 'мальчик/сын/брат разбил']
semantic_components_entities = ['девочка/дочка', 'мама/тетя', 'мальчик/сын/брат', 'чашка']
semantic_components_actions = ['ругает', 'разбил']
semantic_components3 = Semantic_components(semantic_components_full, semantic_components_entities, semantic_components_actions)

semantic_components_full =  ['строил/делал мальчик/ребёнок', 'строил/делал башня/пирамидка', 'разрушилась/сломалась/упала башня/пирамидка/она']
semantic_components_entities = ['мальчик/ребёнок', 'башня/пирамидка', 'ещё/другая/новая']
semantic_components_actions = ['строил/делал', 'разрушилась/сломалась/упала']
semantic_components1 = Semantic_components(semantic_components_full, semantic_components_entities, semantic_components_actions)

clusters = []
for df, postfix, semantic_components in zip([df1, df2, df3], list(range(1, 4)), [semantic_components1, semantic_components2, semantic_components3]):
  df_cor = get_df(df, str(postfix), semantic_components)
  clusters.append(get_clusters(df_cor))

# df, postfix, semantic_components = df2, '2', semantic_components2
# df_cor = get_df(df, str(postfix), semantic_components)
# clusters.append(get_clusters(df_cor))

clusters

clusters[0] = np.append(clusters[0], 1)

list(zip(*clusters))

for i, c in enumerate(zip(*clusters)):
  cnt = Counter(c)
  print(f'{i} - {cnt.most_common(1)[0][0]}')

"""1) девочка
2) мама (тетя)
3) мальчик
4) две чашки на столе
5) на полу осколки чашки
6) мама ругает девочку
7) девочка удивлена
8) мальчик прячется
9) у мальчика блюдце
10) это мальчик разбил чашку
"""

df2

semantic_components_full = ['кошка', 'собака', 'навредить', 'сбросил/уронил цветок/горшок', 'упал грабли', ['грабли стукнули/кошке/дала', 'стукнули палка', 'кошке палка']]
semantic_components_entities = ['кошка', 'собака', 'цветок/горшок', 'грабли/палка/щетка']
semantic_components_actions = ['уронить/сбросить', 'упал', 'стукнуть/дала']
# semantic_components2 = Semantic_components(semantic_components_full, semantic_components_entities, semantic_components_actions)

df_cor = create_attrs(df2['tell2'], (semantic_components_full, semantic_components_entities, semantic_components_actions)).iloc[:, 1:]
df_cor['Время'] = df2['time2']
df_cor['is_strong'] = df2['is_strong']
plt.figure(figsize=(10,5))
sns.heatmap(df_cor.corr(), annot = True, vmin=-1, vmax=1, center= 0)
# df_cor

df_cor = df_cor.add_suffix('1')
df_cor

df_cor_all = pd.concat([df_cor, df_cor], axis=1) #ИЗМЕНЕНО df_cor_all -> df_cor
df_cor_all

plt.figure(figsize=(10,5))
sns.heatmap(df_cor.corr(), annot = True, vmin=-1, vmax=1, center= 0)
# df_cor

from sklearn import preprocessing
from sklearn.cluster import KMeans, SpectralClustering

scaler = preprocessing.StandardScaler().fit(df_cor.fillna(0))
df_cor_scaled = scaler.transform(df_cor.fillna(0))

clustering = KMeans(n_clusters=2, random_state=0, n_init=10).fit_predict(df_cor_scaled)
print(clustering)

clustering = SpectralClustering(n_clusters=2, n_neighbors=2, n_init=1, n_components=2, affinity='nearest_neighbors').fit_predict(df_cor_scaled)
print(clustering)
print(len(clustering))

clustering = pd.DataFrame()

kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)
clustering['clustering'] = kmeans.fit_predict(df_cor_scaled)
print(clustering)


centroids = kmeans.cluster_centers_
cen_x = [i[0] for i in centroids]
cen_y = [i[1] for i in centroids]
cen_x

## add to df
clustering['cen_x'] = clustering['clustering'].map({0:cen_x[0], 1:cen_x[1]})
clustering['cen_y'] = clustering['clustering'].map({0:cen_y[0], 1:cen_y[1]})
# define and map colors
colors = ['#DF2020', '#81DF20', '#2095DF']
clustering['c'] = clustering.clustering.map({0:colors[0], 1:colors[1]})

import matplotlib.pyplot as plt
plt.scatter(df_cor['Проц глаг1'], df_cor['Семант полнота1'], c=clustering.c, alpha = 0.6, s=10)

"""## Датасет школьников

1 птица-мама;  2 птенцы;  3 котик;  4 хочет схватить птенца;  5 кормит птенцов;  6 отвернулась;  7 собака;  8 схватила кота;  9 спасла птенцов;  10 прогнала кота)
"""

['птица', 'птенцы', 'котик', 'схватить птенца', 'свить гнездо', 'кормит птенцов', 'отвернулась', 'собака', 'схватила кота',
                         'спасла/кормит птенцов', ['прогнала кота', 'кормит птенцов']]

semantic_components_full =  ['птица/мама', 'птенцы/дети', 'кот', 'схватить птенца', 'кормит птенцы/дети', 'птица/мама отвернулась/улетела', 'собака', 'схватила кота',
                             'спасла птенцов', 'прогнала кота']

semantic_components_entities = ['птица/мама', 'птенцы/дети', 'кот', 'гнездо', 'собака']
semantic_components_actions = ['кормить', 'отвернулась/улетела', 'схватила', 'спасла', 'прогнала']
print(df)
df_cor = create_attrs(df['tell3'].sample(n=50, replace=True), (semantic_components_full, semantic_components_entities, semantic_components_actions)).iloc[:, 1:]
df_cor['time3'] = df['time3']
plt.figure(figsize=(10,5))
sns.heatmap(df_cor.corr(), annot = True, vmin=-1, vmax=1, center= 0)
# df_cor

df_cor

df_cor[["time3"]] = df_cor[["time3"]].astype(str)
df_cor['time3'] = df_cor['time3'].map(lambda s: re.sub('\D', '', s))
df_cor = df_cor[df_cor.time3 != '']
scaler = preprocessing.StandardScaler().fit(df_cor)
df_scaled = scaler.transform(df_cor)

clustering = pd.DataFrame()

kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)
clustering['clustering'] = kmeans.fit_predict(df_scaled)
print(clustering)


centroids = kmeans.cluster_centers_
cen_x = [i[0] for i in centroids]
cen_y = [i[1] for i in centroids]
cen_x

## add to df
clustering['cen_x'] = clustering['clustering'].map({0:cen_x[0], 1:cen_x[1]})
clustering['cen_y'] = clustering['clustering'].map({0:cen_y[0], 1:cen_y[1]})
# define and map colors
colors = ['#DF2020', '#81DF20', '#2095DF']
clustering['c'] = clustering.clustering.map({0:colors[0], 1:colors[1]})

import matplotlib.pyplot as plt
plt.scatter(df_cor['Проц глаг'], df_cor['Семант полнота'], c=clustering.c, alpha = 0.6, s=10)

from sklearn import preprocessing
from sklearn.cluster import KMeans, SpectralClustering

df_cor[["time3"]] = df_cor[["time3"]].astype(str)
df_cor['time3'] = df_cor['time3'].map(lambda s: re.sub('\D', '', s))
df_cor = df_cor[df_cor.time3 != '']
scaler = preprocessing.StandardScaler().fit(df_cor)
df_scaled = scaler.transform(df_cor)

clustering = KMeans(n_clusters=2, random_state=0).fit_predict(df_scaled)
plt.figure(figsize=(10,5))
#sns.heatmap(pd.concat([df_cor, pd.Series(clustering)], axis=1).corr(), annot = True, vmin=-1, vmax=1, center= 0)
sns.heatmap(pd.concat([df_cor, pd.Series(clustering)]).corr(), annot = True, vmin=-1, vmax=1, center= 0)

spectral_clustering = SpectralClustering(n_clusters=2, n_neighbors=2, n_init=1, n_components=2, affinity='nearest_neighbors')
clustering = spectral_clustering.fit_predict(df_scaled)
print(clustering)

#pd.concat([df_cor, pd.Series(clustering)], axis=1)
pd.concat([df_cor, pd.Series(clustering)])
plt.figure(figsize=(10,5))
#sns.heatmap(pd.concat([df_cor, pd.Series(clustering)], axis=1).corr(), annot = True, vmin=-1, vmax=1, center= 0)
sns.heatmap(pd.concat([df_cor, pd.Series(clustering)]).corr(), annot = True, vmin=-1, vmax=1, center= 0)